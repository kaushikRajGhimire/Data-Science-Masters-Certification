{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb5gb3N184s3PeMujL12eJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaushikRajGhimire/Data-Science-Masters-Certification/blob/main/13_October_BatchNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Explain the concept of batch normalization in the context of Artificial Neural Networks\n",
        "Batch normalization (BatchNorm) is a technique used to improve the training of artificial neural networks by normalizing the input of each layer to have a mean of zero and a standard deviation of one within a mini-batch. This helps mitigate the problem of internal covariate shift, where the distribution of inputs to a layer changes during training, leading to faster and more stable training.\n",
        "\n",
        "#2. Describe the benefits of using batch normalization during training\n",
        "Batch normalization offers several key benefits during training:\n",
        "\n",
        "Faster convergence: By normalizing activations, it allows the model to learn faster, reducing the time needed to train deep networks.\n",
        "Reduces dependence on careful weight initialization: Since activations are normalized, the model becomes less sensitive to the starting weights.\n",
        "Reduces the need for dropout or other regularization techniques: It has a regularizing effect, reducing overfitting even in large models.\n",
        "Stabilizes training: It reduces the internal covariate shift, preventing activation values from becoming too large or too small, which helps stabilize the training process.\n",
        "#3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters\n",
        "Batch normalization works in two main steps:\n",
        "\n",
        "Normalization step: For each mini-batch, the input activations are normalized by subtracting the batch mean and dividing by the batch standard deviation. This ensures that the inputs to each layer have a mean of 0 and a variance of 1.\n",
        "\n",
        "Learnable parameters: After normalization, two trainable parameters, gamma (scaling) and beta (shifting), are introduced. These allow the network to restore the capacity to represent the original data distribution if needed. Gamma scales the normalized data, and beta shifts it, allowing the network to learn an appropriate transformation for the task."
      ],
      "metadata": {
        "id": "x5OEeG5qgKnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Preprocessing - Normalize the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to mean=0.5 and std=0.5 for all 3 channels\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOBj8e_mgvxt",
        "outputId": "29459eda-3808-4123-d687-503603b18696"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 86724979.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the feedforward neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)  # Flatten the input image\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "bsEouqJxhvk5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print(f'Epoch {epoch + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "# Train without Batch Normalization\n",
        "train_model(model, epochs=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pPrpt0Lh4_L",
        "outputId": "138ca885-50ef-4225-8362-3377b7f3fb0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.3647116333246232\n",
            "Epoch 1, Loss: 1.4188485765457153\n",
            "Epoch 1, Loss: 1.4105649638175963\n",
            "Epoch 1, Loss: 1.4282653892040253\n",
            "Epoch 1, Loss: 1.411482927799225\n",
            "Epoch 1, Loss: 1.4318719732761382\n",
            "Epoch 1, Loss: 1.3920927035808563\n",
            "Epoch 2, Loss: 1.2829652428627014\n",
            "Epoch 2, Loss: 1.2794200658798218\n",
            "Epoch 2, Loss: 1.3095080786943436\n",
            "Epoch 2, Loss: 1.3162622594833373\n",
            "Epoch 2, Loss: 1.3175205636024474\n",
            "Epoch 2, Loss: 1.2972543412446975\n",
            "Epoch 2, Loss: 1.2915997797250747\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNNWithBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNNWithBN, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)  # Batch Normalization for first layer\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)  # Batch Normalization for second layer\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)  # Flatten input image\n",
        "        x = self.relu(self.bn1(self.fc1(x)))  # Add batch norm after each layer\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model with Batch Normalization\n",
        "model_with_bn = SimpleNNWithBN()\n",
        "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "1J07kgxDiOTg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with Batch Normalization\n",
        "train_model(model_with_bn, epochs=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AabrTWdNiWpR",
        "outputId": "a9bc18b7-b7af-4cf3-c8e9-f4d563756ee3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.370866253376007\n",
            "Epoch 1, Loss: 2.370631902217865\n",
            "Epoch 1, Loss: 2.374221239089966\n",
            "Epoch 1, Loss: 2.3667554664611816\n",
            "Epoch 1, Loss: 2.369982900619507\n",
            "Epoch 1, Loss: 2.3660445165634156\n",
            "Epoch 1, Loss: 2.3700245594978333\n",
            "Epoch 2, Loss: 2.372561204433441\n",
            "Epoch 2, Loss: 2.3658617520332337\n",
            "Epoch 2, Loss: 2.367136785984039\n",
            "Epoch 2, Loss: 2.373651797771454\n",
            "Epoch 2, Loss: 2.3643780398368834\n",
            "Epoch 2, Loss: 2.3698064923286437\n",
            "Epoch 2, Loss: 2.3735621070861814\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Evaluate both models\n",
        "print(\"Without Batch Normalization:\")\n",
        "evaluate_model(model)\n",
        "\n",
        "print(\"\\nWith Batch Normalization:\")\n",
        "evaluate_model(model_with_bn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrR1DDb7ivEb",
        "outputId": "8a500d2f-8405-46b3-c609-f59143095eef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Batch Normalization:\n",
            "Accuracy: 51.21%\n",
            "\n",
            "With Batch Normalization:\n",
            "Accuracy: 8.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLoyQ75Jia_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Discuss the Impact of Batch Normalization on Training and Performance\n",
        "Training Speed: Batch normalization often allows for faster convergence due to more stable gradient flow.\n",
        "\n",
        "Regularization: Adding batch normalization typically reduces overfitting, making it easier to generalize to the test set even without other regularization techniques like dropout.\n",
        "\n",
        "Model Accuracy: With batch normalization, the model’s validation accuracy is expected to improve due to the smoother learning process.\n",
        "\n",
        "Loss Behavior: You will likely observe lower training and validation loss in the model with batch normalization, as it mitigates the internal covariate shift, leading to more stable training dynamics."
      ],
      "metadata": {
        "id": "uqqKrDZpjHeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with different batch sizes\n",
        "def train_with_batch_size(model, batch_size, epochs):\n",
        "    # Redefine DataLoader with new batch size\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print(f'Epoch {epoch + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "# Experiment with different batch sizes (e.g., 32, 64, 128)\n",
        "for batch_size in [32, 64, 128]:\n",
        "    print(f'\\nTraining with batch size {batch_size}')\n",
        "    model_with_bn = SimpleNNWithBN()  # Reinitialize model\n",
        "    train_with_batch_size(model_with_bn, batch_size=batch_size, epochs=10)\n",
        "    evaluate_model(model_with_bn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCHSTqNYjXsF",
        "outputId": "f345a575-d9c2-4262-9f2b-dea78314f7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with batch size 32\n",
            "Epoch 1, Loss: 1.913565467596054\n",
            "Epoch 1, Loss: 1.8033307909965515\n",
            "Epoch 1, Loss: 1.7290558671951295\n",
            "Epoch 1, Loss: 1.6747472035884856\n",
            "Epoch 1, Loss: 1.677281427383423\n",
            "Epoch 1, Loss: 1.6465860414505005\n",
            "Epoch 1, Loss: 1.6301004528999328\n",
            "Epoch 1, Loss: 1.5884240102767944\n",
            "Epoch 1, Loss: 1.5878562259674072\n",
            "Epoch 1, Loss: 1.5628017783164978\n",
            "Epoch 1, Loss: 1.5734307515621184\n",
            "Epoch 1, Loss: 1.54383612036705\n",
            "Epoch 1, Loss: 1.5293263006210327\n",
            "Epoch 1, Loss: 1.5209683179855347\n",
            "Epoch 1, Loss: 1.5514789819717407\n",
            "Epoch 2, Loss: 1.4144401091337204\n",
            "Epoch 2, Loss: 1.424605959057808\n",
            "Epoch 2, Loss: 1.4636397898197173\n",
            "Epoch 2, Loss: 1.464867570400238\n",
            "Epoch 2, Loss: 1.4236979669332503\n",
            "Epoch 2, Loss: 1.454768923521042\n",
            "Epoch 2, Loss: 1.4217901968955993\n",
            "Epoch 2, Loss: 1.4666713178157806\n",
            "Epoch 2, Loss: 1.4620471835136413\n",
            "Epoch 2, Loss: 1.4218910336494446\n",
            "Epoch 2, Loss: 1.4047271049022674\n",
            "Epoch 2, Loss: 1.4027776503562928\n",
            "Epoch 2, Loss: 1.3923697638511658\n",
            "Epoch 2, Loss: 1.4167664456367492\n",
            "Epoch 2, Loss: 1.4339658105373383\n",
            "Epoch 3, Loss: 1.3399257892370224\n",
            "Epoch 3, Loss: 1.3284146124124527\n",
            "Epoch 3, Loss: 1.3374200195074082\n",
            "Epoch 3, Loss: 1.305475794672966\n",
            "Epoch 3, Loss: 1.3617448914051056\n",
            "Epoch 3, Loss: 1.3329779040813445\n",
            "Epoch 3, Loss: 1.320861937403679\n",
            "Epoch 3, Loss: 1.3349665492773055\n",
            "Epoch 3, Loss: 1.3236541777849198\n",
            "Epoch 3, Loss: 1.336451876759529\n",
            "Epoch 3, Loss: 1.3321123677492142\n",
            "Epoch 3, Loss: 1.3282702726125717\n",
            "Epoch 3, Loss: 1.3270824980735778\n",
            "Epoch 3, Loss: 1.3331588524580003\n",
            "Epoch 3, Loss: 1.3162268567085267\n",
            "Epoch 4, Loss: 1.23806706905365\n",
            "Epoch 4, Loss: 1.2393031048774719\n",
            "Epoch 4, Loss: 1.2477771252393723\n",
            "Epoch 4, Loss: 1.238613429069519\n",
            "Epoch 4, Loss: 1.2815898698568344\n",
            "Epoch 4, Loss: 1.2653076648712158\n",
            "Epoch 4, Loss: 1.2841691362857819\n",
            "Epoch 4, Loss: 1.2397559064626693\n",
            "Epoch 4, Loss: 1.2780188596248627\n",
            "Epoch 4, Loss: 1.2621083784103393\n",
            "Epoch 4, Loss: 1.239357862472534\n",
            "Epoch 4, Loss: 1.2230010724067688\n",
            "Epoch 4, Loss: 1.249424043893814\n",
            "Epoch 4, Loss: 1.2676996028423309\n",
            "Epoch 4, Loss: 1.2598343253135682\n",
            "Epoch 5, Loss: 1.1517610818147659\n",
            "Epoch 5, Loss: 1.1867576336860657\n",
            "Epoch 5, Loss: 1.1741522192955016\n",
            "Epoch 5, Loss: 1.184330940246582\n",
            "Epoch 5, Loss: 1.1774941331148148\n",
            "Epoch 5, Loss: 1.1796010315418244\n",
            "Epoch 5, Loss: 1.1778326314687728\n",
            "Epoch 5, Loss: 1.2063794898986817\n",
            "Epoch 5, Loss: 1.2181782269477843\n",
            "Epoch 5, Loss: 1.220970670580864\n",
            "Epoch 5, Loss: 1.1802817898988724\n",
            "Epoch 5, Loss: 1.1952098876237869\n",
            "Epoch 5, Loss: 1.2018279039859772\n",
            "Epoch 5, Loss: 1.1568317133188248\n",
            "Epoch 5, Loss: 1.1533454072475433\n",
            "Epoch 6, Loss: 1.1066098183393478\n",
            "Epoch 6, Loss: 1.0971842801570892\n",
            "Epoch 6, Loss: 1.145596246123314\n",
            "Epoch 6, Loss: 1.0974485313892364\n",
            "Epoch 6, Loss: 1.1201690250635148\n",
            "Epoch 6, Loss: 1.1345026683807373\n",
            "Epoch 6, Loss: 1.11955719769001\n",
            "Epoch 6, Loss: 1.1540937560796738\n",
            "Epoch 6, Loss: 1.140988315343857\n",
            "Epoch 6, Loss: 1.1462168669700623\n",
            "Epoch 6, Loss: 1.1192581921815872\n",
            "Epoch 6, Loss: 1.1381902652978897\n",
            "Epoch 6, Loss: 1.1462043207883834\n",
            "Epoch 6, Loss: 1.1314661294221877\n",
            "Epoch 6, Loss: 1.0888333868980409\n",
            "Epoch 7, Loss: 1.011576628088951\n",
            "Epoch 7, Loss: 1.0607449662685395\n",
            "Epoch 7, Loss: 1.0297903621196747\n",
            "Epoch 7, Loss: 1.0320139235258103\n",
            "Epoch 7, Loss: 1.048827025294304\n",
            "Epoch 7, Loss: 1.0682679688930512\n",
            "Epoch 7, Loss: 1.0581394070386887\n",
            "Epoch 7, Loss: 1.0813699465990068\n",
            "Epoch 7, Loss: 1.09540895819664\n",
            "Epoch 7, Loss: 1.090065615773201\n",
            "Epoch 7, Loss: 1.098339132666588\n",
            "Epoch 7, Loss: 1.0906570029258729\n",
            "Epoch 7, Loss: 1.02971635222435\n",
            "Epoch 7, Loss: 1.0612703162431716\n",
            "Epoch 7, Loss: 1.040135219693184\n",
            "Epoch 8, Loss: 0.9479984414577484\n",
            "Epoch 8, Loss: 0.9888156396150589\n",
            "Epoch 8, Loss: 0.984392991065979\n",
            "Epoch 8, Loss: 0.9926800829172134\n",
            "Epoch 8, Loss: 1.0194974595308304\n",
            "Epoch 8, Loss: 1.009507822394371\n",
            "Epoch 8, Loss: 1.0106756281852722\n",
            "Epoch 8, Loss: 0.9911702978610992\n",
            "Epoch 8, Loss: 1.031100764274597\n",
            "Epoch 8, Loss: 0.988193170428276\n",
            "Epoch 8, Loss: 1.0336861580610275\n",
            "Epoch 8, Loss: 1.0364715909957887\n",
            "Epoch 8, Loss: 1.0511475813388824\n",
            "Epoch 8, Loss: 1.030024167895317\n",
            "Epoch 8, Loss: 1.0178203535079957\n",
            "Epoch 9, Loss: 0.9267838403582573\n",
            "Epoch 9, Loss: 0.9380466824769974\n",
            "Epoch 9, Loss: 0.931058732867241\n",
            "Epoch 9, Loss: 0.9151481038331986\n",
            "Epoch 9, Loss: 0.9381290858983994\n",
            "Epoch 9, Loss: 0.9502754127979278\n",
            "Epoch 9, Loss: 1.023506653904915\n",
            "Epoch 9, Loss: 0.9403479319810867\n",
            "Epoch 9, Loss: 0.9900751858949661\n",
            "Epoch 9, Loss: 0.9456796145439148\n",
            "Epoch 9, Loss: 0.9441128551959992\n",
            "Epoch 9, Loss: 0.9908817654848099\n",
            "Epoch 9, Loss: 0.9656662532687187\n",
            "Epoch 9, Loss: 0.9356035822629929\n",
            "Epoch 9, Loss: 0.944766319990158\n",
            "Epoch 10, Loss: 0.8674597173929215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advantages of Batch Normalization:\n",
        "Faster Convergence: Batch normalization reduces internal covariate shift, which stabilizes training and speeds up convergence. It allows for higher learning rates without the risk of divergence.\n",
        "\n",
        "Better Regularization: By introducing noise to the training process through mini-batch statistics, batch normalization has a slight regularization effect, often reducing the need for dropout or other regularization techniques.\n",
        "\n",
        "Reduces Sensitivity to Initialization: With batch normalization, the network becomes less sensitive to the choice of initial weights. It ensures that the activations do not become too large or too small, leading to more consistent and reliable training.\n",
        "\n",
        "Higher Learning Rates: Models with batch normalization can use higher learning rates, which can lead to faster convergence without exploding gradients.\n",
        "\n",
        "Improved Gradient Flow: Batch normalization helps prevent the problem of vanishing and exploding gradients in deep networks, ensuring better gradient flow through the network.\n",
        "\n",
        "#Potential Limitations of Batch Normalization:\n",
        "Dependence on Batch Size: Batch normalization relies on batch statistics (mean and variance), making it sensitive to the batch size. For very small batch sizes, it may not perform well because the statistics might not represent the overall data distribution accurately.\n",
        "\n",
        "Extra Computation: Although batch normalization often leads to faster convergence, it adds extra computational overhead due to the need to calculate batch statistics and apply the normalization step at each layer.\n",
        "\n",
        "Inconsistency Between Training and Inference: During training, batch statistics are used, but during inference, global running averages are used. If these running statistics do not generalize well, it can lead to a small drop in performance at test time.\n",
        "\n",
        "Not Always Effective: While batch normalization works well for many models, in some cases, other techniques like Layer Normalization or Group Normalization may be more suitable, especially when batch sizes are too small for effective statistics."
      ],
      "metadata": {
        "id": "m9Zgw3IMj7sZ"
      }
    }
  ]
}