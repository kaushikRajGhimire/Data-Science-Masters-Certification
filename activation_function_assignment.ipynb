{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmA04sCMb5m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. What is an activation function in the context of artificial neural networks?\n",
        "An activation function introduces non-linearity into the neural network, enabling it to learn and model complex patterns by determining whether a neuron should be activated (fired) based on its inputs.\n",
        "\n",
        "# Q2. What are some common types of activation functions used in neural networks?\n",
        "Common activation functions include sigmoid, ReLU (Rectified Linear Unit), tanh (hyperbolic tangent), leaky ReLU, and softmax.\n",
        "\n",
        "# Q3. How do activation functions affect the training process and performance of a neural network?\n",
        "Activation functions influence the training process by controlling the output range of neurons and enabling non-linear transformations. The choice of activation function affects the model's convergence rate, gradient behavior, and overall performance in learning complex relationships.\n",
        "\n",
        "# Q4. How does the sigmoid activation function work? What are its advantages and disadvantages?\n",
        "The sigmoid function squashes input values to a range between 0 and 1, making it useful for binary classification. Its advantages include smooth gradients and output in a probabilistic form. However, it suffers from the vanishing gradient problem, slowing down training in deep networks.\n",
        "\n",
        "# Q5. What is the rectified linear unit (ReLU) activation function? How does it differ from the sigmoid function?\n",
        "ReLU outputs the input directly if positive and zero otherwise, allowing for faster training and mitigating the vanishing gradient issue. Unlike sigmoid, which saturates outputs and compresses values between 0 and 1, ReLU maintains positive outputs without squeezing them, promoting efficient learning.\n",
        "\n",
        "# Q6. What are the benefits of using the ReLU activation function over the sigmoid function?\n",
        "ReLU is computationally simpler and helps mitigate the vanishing gradient problem, allowing models to converge faster. It also allows networks to model complex relationships more effectively by avoiding saturation seen in the sigmoid function.\n",
        "\n",
        "# Q7. Explain the concept of \"leaky ReLU\" and how it addresses the vanishing gradient problem.\n",
        "Leaky ReLU is a variant of ReLU where small negative values are allowed instead of zero for negative inputs, which prevents neurons from dying and helps with the vanishing gradient problem, ensuring better gradient flow during backpropagation.\n",
        "\n",
        "# Q8. What is the purpose of the softmax activation function? When is it commonly used?\n",
        "Softmax converts raw scores into probabilities by normalizing the output across multiple classes, making it ideal for multi-class classification tasks, where the goal is to assign probabilities to different categories.\n",
        "\n",
        "# Q9. What is the hyperbolic tangent (tanh) activation function? How does it compare to the sigmoid function?\n",
        "The tanh function maps inputs to the range of -1 to 1, centering data around zero, unlike sigmoid which outputs between 0 and 1. Tanh generally performs better than sigmoid in deep networks due to less severe saturation, but it still suffers from the vanishing gradient problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lzjWN0Hvcbyj"
      }
    }
  ]
}